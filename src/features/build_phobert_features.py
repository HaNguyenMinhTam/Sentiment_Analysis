import os
import re
import unicodedata
import pandas as pd
import numpy as np
import torch
from tqdm import tqdm
import joblib
from transformers import AutoTokenizer, AutoModel


# =============================
# ‚öôÔ∏è C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N
# =============================
BASE_DIR = "D:/Projects/Sentiment_Analysis"
DATA_DIR = os.path.join(BASE_DIR, "data/interim")        # ƒë·ªçc d·ªØ li·ªáu g·ªëc (ch∆∞a TF-IDF clean)
FEATURE_DIR = os.path.join(BASE_DIR, "data/features/PhoBERT")
LABEL_DIR = os.path.join(BASE_DIR, "models/PhoBERT")

os.makedirs(FEATURE_DIR, exist_ok=True)
os.makedirs(LABEL_DIR, exist_ok=True)


# =============================
# üßπ H√ÄM TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N
# =============================
def preprocess_phobert_text(text):
    """
    Chu·∫©n h√≥a vƒÉn b·∫£n tr∆∞·ªõc khi ƒë∆∞a v√†o PhoBERT:
    - Chu·∫©n Unicode (NFC)
    - Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    - X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ, s·ªë v√† d·∫•u ti·∫øng Vi·ªát
    - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    """
    if pd.isna(text):
        return ""
    text = unicodedata.normalize("NFC", str(text))
    text = text.lower()
    text = re.sub(r'[^0-9a-z√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá'
                  r'√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±'
                  r'·ª≥√Ω·ª∑·ªπ·ªµƒë\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# =============================
# üìñ ƒê·ªåC D·ªÆ LI·ªÜU
# =============================
train_df = pd.read_csv(os.path.join(DATA_DIR, "train.csv"), encoding="utf-8-sig")
dev_df   = pd.read_csv(os.path.join(DATA_DIR, "dev.csv"), encoding="utf-8-sig")
test_df  = pd.read_csv(os.path.join(DATA_DIR, "test.csv"), encoding="utf-8-sig")

print(f"Train: {train_df.shape}, Dev: {dev_df.shape}, Test: {test_df.shape}")

# L√†m s·∫°ch c·ªôt 'sentence'
for df in [train_df, dev_df, test_df]:
    df["sentence_clean"] = df["sentence"].apply(preprocess_phobert_text)


# =============================
# ü§ñ LOAD PHOBERT MODEL
# =============================
model_name = "vinai/phobert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print(f"Using device: {device}")


# =============================
# üî¢ H√ÄM SINH EMBEDDING
# =============================
def get_phobert_embedding(texts, batch_size=8, max_length=256, pooling="cls"):
    """
    T·∫°o embedding PhoBERT t·ª´ danh s√°ch c√¢u.
    - pooling: 'cls' ho·∫∑c 'mean'
    """
    embeddings = []
    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Generating PhoBERT embeddings"):
            batch_texts = texts[i:i+batch_size]
            encoded = tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            ).to(device)

            outputs = model(**encoded)
            hidden = outputs.last_hidden_state

            if pooling == "cls":
                batch_embed = hidden[:, 0, :].cpu().numpy()
            elif pooling == "mean":
                batch_embed = hidden.mean(dim=1).cpu().numpy()
            else:
                raise ValueError("pooling ph·∫£i l√† 'cls' ho·∫∑c 'mean'")

            embeddings.append(batch_embed)

    return np.vstack(embeddings)


# =============================
# üöÄ T·∫†O EMBEDDING CHO 3 T·∫¨P
# =============================
print("B·∫Øt ƒë·∫ßu tr√≠ch xu·∫•t embedding PhoBERT...")

X_train_pho = get_phobert_embedding(train_df["sentence_clean"].tolist(), batch_size=8, max_length=256)
X_dev_pho   = get_phobert_embedding(dev_df["sentence_clean"].tolist(), batch_size=8, max_length=256)
X_test_pho  = get_phobert_embedding(test_df["sentence_clean"].tolist(), batch_size=8, max_length=256)

print("‚úÖ ƒê√£ sinh xong embedding PhoBERT!")


# =============================
# üíæ L∆ØU EMBEDDING & LABEL
# =============================
np.savez_compressed(os.path.join(FEATURE_DIR, "X_train_phobert.npz"), X_train_pho)
np.savez_compressed(os.path.join(FEATURE_DIR, "X_dev_phobert.npz"), X_dev_pho)
np.savez_compressed(os.path.join(FEATURE_DIR, "X_test_phobert.npz"), X_test_pho)

joblib.dump(train_df["sentiment"].values, os.path.join(LABEL_DIR, "y_train.pkl"))
joblib.dump(dev_df["sentiment"].values, os.path.join(LABEL_DIR, "y_dev.pkl"))
joblib.dump(test_df["sentiment"].values, os.path.join(LABEL_DIR, "y_test.pkl"))

print("üéØ Embedding & labels ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i:", FEATURE_DIR)
